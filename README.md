# ğŸ”¬ Argus â€” Autonomous Deep Research Engine

> A **production-grade multi-agent research pipeline** that autonomously plans, researches, critiques, and synthesizes comprehensive cited reports from any research query.

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115-green.svg)](https://fastapi.tiangolo.com)
[![LangGraph](https://img.shields.io/badge/LangGraph-multi--agent-orange.svg)](https://langchain-ai.github.io/langgraph/)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://docker.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## What Is Argus?

Argus accepts a research query via REST API and runs a **supervisor-orchestrated multi-agent pipeline** that:

1. **Plans** â€” breaks the query into focused sub-questions
2. **Researches** â€” searches the web (Tavily), finds papers (ArXiv), and looks up background (Wikipedia)
3. **Critiques** â€” reviews its own findings for gaps and loops back if needed
4. **Writes** â€” synthesizes a structured markdown report with numbered citations

The entire pipeline runs **asynchronously** â€” the API returns a `job_id` immediately and the client polls for completion. Research jobs are persisted in SQLite. Every LLM call and agent turn is traced in LangSmith.

---

## Architecture

```
User HTTP Request
      â”‚
POST /research  (FastAPI â€” async, returns job_id immediately)
      â”‚
Creates Job (UUID) â”€â”€â–º SQLite jobs table
      â”‚ (background thread)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Supervisor Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reads state, decides next agent via Command(goto=...) routing  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ delegates to
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Planner  â”‚  â”‚ Researcher â”‚  â”‚  Critic  â”‚  â”‚    Writer    â”‚
â”‚          â”‚  â”‚            â”‚  â”‚          â”‚  â”‚              â”‚
â”‚ Breaks   â”‚  â”‚ Tavily +   â”‚  â”‚ Reviews  â”‚  â”‚ Synthesizes  â”‚
â”‚ query    â”‚  â”‚ ArXiv +    â”‚  â”‚ gaps +   â”‚  â”‚ final        â”‚
â”‚ into     â”‚  â”‚ Wikipedia  â”‚  â”‚ requests â”‚  â”‚ markdown     â”‚
â”‚ sub-Qs   â”‚  â”‚            â”‚  â”‚ more     â”‚  â”‚ report with  â”‚
â”‚          â”‚  â”‚            â”‚  â”‚ research â”‚  â”‚ citations    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚
                                          Job result â”€â”€â–º SQLite
                                                     â”‚
GET /jobs/{id}/status  â”€â”€â–º polls until complete
GET /jobs/{id}/result  â”€â”€â–º returns full markdown report
      â”‚
LangSmith traces entire run (observability)
      â”‚
Streamlit UI reads API â”€â”€â–º displays report
```

### State Machine (LangGraph)

```
START
  â”‚
[supervisor] â”€â”€â–º planner â”€â”€â–º [supervisor]
                                  â”‚
                             researcher â”€â”€â–º [supervisor]
                                  â”‚
                               critic â”€â”€â–º [supervisor]
                                  â”‚
                    (gaps found AND iterations < 3?)
                         Yes â”€â”€â–º researcher (loop)
                         No  â”€â”€â–º writer â”€â”€â–º [supervisor] â”€â”€â–º END
```

The supervisor uses `Command(goto=...)` routing â€” the **LLM decides the flow** based on agent outputs, not hardcoded chains. `research_iterations >= 3` is enforced in code as a hard safety cap regardless of LLM decisions, preventing infinite loops on the Groq free tier.

### Two Persistence Layers

```
Layer 1 â€” Job Persistence (custom SQLite table)
  jobs table: job_id | query | depth | status | result | error | agent_turns | created_at | updated_at
  status flow: "pending" â”€â”€â–º "running" â”€â”€â–º "complete" | "failed"

Layer 2 â€” LangGraph Checkpoints (SqliteSaver)
  Saves graph state after every node execution
  thread_id = job_id (same UUID reused across both layers)
```

Both layers live in `data/research.db`. Swapping to PostgreSQL is a one-line change in `src/persistence/db.py`.

---

## Tech Stack

| Component | Choice | Why |
|-----------|--------|-----|
| Agent framework | LangGraph supervisor pattern | Native multi-agent, cyclic graph, checkpointing |
| LLM | Groq â€” Llama 3.3 70B Versatile | Free tier, 500+ tok/s, deterministic routing |
| Web search | Tavily | Semantic search with scored, cited results |
| Paper search | ArXiv | Direct library, rate-limit fix applied |
| General knowledge | Wikipedia | Fast encyclopedic background |
| REST API | FastAPI + uvicorn | Async-native, OpenAPI docs auto-generated |
| Async tasks | FastAPI BackgroundTasks | Zero extra deps, sufficient for single-user |
| Persistence | SQLite + LangGraph SqliteSaver | Zero infra, PostgreSQL-ready |
| Observability | LangSmith | Per-agent token counts, latency, tool traces |
| Containerization | Docker + docker-compose | Reproducible builds, Render-ready |
| Deployment | Render (free tier) | Live public URL |
| UI | Streamlit | Polls API, renders markdown report |
| Config | python-dotenv | Standard 12-factor app config |

---

## How This Differs From Single-Agent ReAct

This is architecturally different from a standard ReAct agent:

| Dimension | Single-Agent ReAct | Argus (Multi-Agent Supervisor) |
|-----------|-------------------|-------------------------------|
| Pattern | One LLM loop with tools | Supervisor orchestrates 4 specialist agents |
| Agent count | 1 | 5 (supervisor + planner + researcher + critic + writer) |
| Interface | Streamlit only | FastAPI REST API + Streamlit |
| Task model | Synchronous, blocks | Async jobs, non-blocking |
| Persistence | Conversation history only | Jobs table + LangGraph checkpoints |
| Critique loop | None | Critic agent identifies gaps, loops back |
| Output | Chat reply | Structured markdown report with citations |
| Control flow | LLM `tool_calls` | `Command(goto=agent_name)` routing |
| Observability | None | LangSmith traces every agent turn |
| Deployment | Not containerized | Dockerized, live on Render |

**Core skill demonstrated:** ReAct = tool use. Supervisor = orchestration. Orchestration is the harder, more senior skill.

---

## Project Structure

```
Argus/
â”œâ”€â”€ .env                          # API keys â€” never commit
â”œâ”€â”€ .env.example                  # Template â€” commit this
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ render.yaml                   # Render deployment config
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ ARCHITECTURE.md
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ research.db               # SQLite â€” auto-created on first run
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ api/
    â”‚   â”œâ”€â”€ main.py               # FastAPI app, CORS, lifespan startup
    â”‚   â”œâ”€â”€ models.py             # Pydantic request/response models
    â”‚   â””â”€â”€ routes/
    â”‚       â”œâ”€â”€ research.py       # POST /research, GET /jobs/{id}/status+result
    â”‚       â””â”€â”€ health.py         # GET /health â€” Render health check
    â”‚
    â”œâ”€â”€ agents/
    â”‚   â”œâ”€â”€ supervisor.py         # LLM routing via Command(goto=...)
    â”‚   â”œâ”€â”€ planner.py            # Decomposes query into sub-questions
    â”‚   â”œâ”€â”€ researcher.py         # Calls Tavily + ArXiv + Wikipedia
    â”‚   â”œâ”€â”€ critic.py             # Identifies research gaps
    â”‚   â””â”€â”€ writer.py             # Synthesizes final markdown report
    â”‚
    â”œâ”€â”€ graph/
    â”‚   â”œâ”€â”€ state.py              # ResearchState TypedDict + add_messages reducer
    â”‚   â””â”€â”€ pipeline.py           # Builds + compiles LangGraph StateGraph
    â”‚
    â”œâ”€â”€ tools/
    â”‚   â”œâ”€â”€ tavily_tool.py        # Web search (Tavily)
    â”‚   â”œâ”€â”€ arxiv_tool.py         # Paper search (ArXiv, rate-limit fix applied)
    â”‚   â””â”€â”€ wikipedia_tool.py     # Background knowledge (Wikipedia)
    â”‚
    â”œâ”€â”€ persistence/
    â”‚   â”œâ”€â”€ db.py                 # SQLite CRUD â€” create_job, update_job_status, get_job
    â”‚   â””â”€â”€ checkpointer.py       # LangGraph SqliteSaver
    â”‚
    â””â”€â”€ ui/
        â””â”€â”€ streamlit_app.py      # Calls FastAPI REST API, polls + renders report
```

---

## API Reference

### `POST /research`
Submit a new research job. Returns immediately with a `job_id`.

```json
// Request
{
  "query": "What are the latest breakthroughs in protein folding AI?",
  "depth": "standard"
}
// depth: "quick" (~20s, 2 sub-questions, web only)
//        "standard" (~45s, 3 sub-questions, web + arxiv + wikipedia)
//        "deep" (~90s, 5 sub-questions, all tools, more results)

// Response â€” 202 Accepted
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "estimated_seconds": 45
}
```

### `GET /jobs/{job_id}/status`
Poll for job progress.

```json
{
  "job_id": "550e8400-...",
  "status": "running",
  "created_at": "2026-02-26T07:00:00Z",
  "updated_at": "2026-02-26T07:00:32Z"
}
// status values: pending | running | complete | failed
```

### `GET /jobs/{job_id}/result`
Fetch the completed report.

```json
{
  "job_id": "550e8400-...",
  "query": "What are the latest breakthroughs in protein folding AI?",
  "status": "complete",
  "report": "## Protein Folding AI: 2025-2026 Breakthroughs\n\n...",
  "sources": ["https://...", "https://arxiv.org/abs/..."],
  "agent_turns": 4,
  "error": null,
  "created_at": "2026-02-26T07:00:00Z",
  "updated_at": "2026-02-26T07:00:38Z"
}
```

### `GET /health`
```json
{ "status": "ok", "version": "1.0.0" }
```

Interactive docs available at `/docs` (Swagger UI auto-generated by FastAPI).

---

## Shared State (ResearchState)

All agents read from and write back to a single `TypedDict` that flows through the graph:

```python
class ResearchState(TypedDict):
    query: str                               # Original research query
    depth: str                               # "quick" | "standard" | "deep"
    messages: Annotated[list, add_messages]  # Full message history â€” add_messages REDUCER
    sub_questions: list[str]                 # Set by Planner
    research_findings: list[str]             # Accumulated by Researcher
    gaps_identified: list[str]               # Set by Critic
    research_iterations: int                 # Incremented by Researcher â€” loop guard
    final_report: str                        # Set by Writer
    sources: list[str]                       # Accumulated throughout
    next_agent: str                          # Set by Supervisor for routing
```

`messages` uses the `add_messages` reducer â€” every agent appends to the history rather than overwriting it. All other fields use default last-write-wins replacement.

---

## Setup & Running Locally

### Prerequisites
- Python 3.11+
- Docker Desktop (for containerized run)
- API keys: [Groq](https://console.groq.com) (free), [Tavily](https://tavily.com) (free), [LangSmith](https://smith.langchain.com) (free, optional)

### 1. Clone and configure

```bash
git clone https://github.com/noviciusss/Argus.git
cd Argus
cp .env.example .env
# Edit .env and add your API keys
```

### 2. Run with Docker (recommended)

```bash
docker-compose up --build
```

- Streamlit UI: `http://localhost:8501`
- API docs: `http://localhost:8000/docs`
- Health check: `http://localhost:8000/health`

### 3. Run without Docker

```bash
pip install -r requirements.txt

# Terminal 1 â€” API
python -m uvicorn src.api.main:app --reload --port 8000

# Terminal 2 â€” UI
python -m streamlit run src/ui/streamlit_app.py
```

> **Always run from the project root** using `python -m` so `src.*` imports resolve correctly.

---

## Design Decisions & Trade-offs

<details>
<summary><strong>Why multi-agent instead of one big ReAct agent?</strong></summary>

A single ReAct agent conflates planning, researching, critiquing, and writing â€” each has different failure modes and requires different prompting strategies. With one agent:
- Planning prompt interferes with tool-calling prompt
- No clean separation of concerns for debugging
- The critique loop is architecturally impossible â€” the agent can't objectively review its own just-completed output in the same turn

Separating into specialist agents allows independent prompts, independent error handling, and a dedicated Critic that reviews findings with fresh context before writing begins.

</details>

<details>
<summary><strong>Why async jobs instead of a streaming/blocking response?</strong></summary>

Research takes 30â€“90 seconds. Standard HTTP requests timeout at ~30 seconds in most clients, browsers, and load balancers. The async job pattern (submit â†’ poll â†’ fetch) decouples request handling from computation â€” this is the standard production pattern for any long-running AI task. It's the same pattern used by OpenAI's Batch API and Anthropic's async endpoints.

An alternative would be Server-Sent Events (SSE) for real-time streaming â€” listed as a future improvement.

</details>

<details>
<summary><strong>Why SQLite instead of PostgreSQL?</strong></summary>

This is a single-user, single-process deployment. SQLite handles this workload easily with zero infrastructure overhead â€” no separate database container, no connection pool, no migration tooling needed. The swap to PostgreSQL is explicitly one line in `src/persistence/db.py` (the connection string). The rest of the code is identical. This was a deliberate design choice to demonstrate production thinking on a dev setup.

</details>

<details>
<summary><strong>Why Groq (Llama 3.3 70B) instead of GPT-4 or Claude?</strong></summary>

Groq's free tier provides ~500 tokens/second â€” fast enough that agent turns feel snappy rather than laggy. For supervisor routing (which needs precise instruction-following), Llama 3.3 70B is sufficiently capable. For a demo project with real usage, paying $0 vs paying per token matters. The LLM is abstracted behind LangChain's `ChatGroq` interface â€” swapping to GPT-4o is one line change in each agent file.

</details>

<details>
<summary><strong>Why FastAPI BackgroundTasks instead of Celery/Redis?</strong></summary>

FastAPI's `BackgroundTasks` requires zero extra infrastructure â€” no Redis container, no worker process, no broker configuration. For single-user usage it works perfectly. The trade-off is that background tasks are in-process, so if the server restarts mid-research, the job is lost (status stays "running" forever in the DB). For a demo portfolio project this is acceptable. Celery + Redis is listed as the production upgrade path.

</details>

<details>
<summary><strong>Why is the checkpointer using a raw SQLite connection instead of from_conn_string()?</strong></summary>

`SqliteSaver.from_conn_string()` returns a context manager designed for `with` blocks â€” it closes the connection when exiting the context. Since the graph lives for the entire app lifetime (built once at module load), the connection must stay open. Passing a raw `sqlite3.connect()` connection directly to `SqliteSaver(conn)` keeps the connection open for the app's lifetime.

</details>

<details>
<summary><strong>Why does depth="quick" skip ArXiv and Wikipedia?</strong></summary>

ArXiv's rate-limit fix requires a 3-second sleep between paper fetches. For a "quick" research run, adding 6â€“9 seconds of sleep per iteration defeats the purpose. Quick mode uses Tavily web search only (3 results) â€” fast but sufficient for general queries. Standard and deep modes enable all three tools.

</details>

<details>
<summary><strong>Render free tier cold starts</strong></summary>

Render's free tier spins containers down after 15 minutes of inactivity. The first request after a cold start takes 30â€“60 seconds to respond â€” this is a Render free-tier limitation, not an application bug. Subsequent requests are fast. The fix is upgrading to a paid Render instance ($7/month) or using a cron job to ping `/health` every 14 minutes to keep the container warm.

</details>

<details>
<summary><strong>What would you add with more time?</strong></summary>

| Improvement | Why |
|-------------|-----|
| Redis + Celery | Proper async task queue â€” jobs survive server restarts |
| PostgreSQL | Multi-user support, persistent jobs across deploys |
| Server-Sent Events (SSE) | Real-time streaming of agent progress instead of polling |
| PDF export | Download research reports as formatted PDFs |
| Rate limiting middleware | `slowapi` â€” prevent free-tier quota abuse |
| LLM-as-Judge evaluation | Score report quality using `DoCopilot` eval pattern |
| Authentication | API key auth for the REST API |
| Report caching | Same query within 24h returns cached result, no API cost |

</details>

---

## Observability

Every LLM call, tool call, and agent turn is automatically traced in **LangSmith** â€” no code instrumentation needed, just env vars.

Set in `.env`:
```bash
LANGSMITH_API_KEY=your_key
LANGSMITH_PROJECT=deep-research-engine
LANGSMITH_TRACING_V2=true
```

After a research run, visit `smith.langchain.com` â†’ `deep-research-engine` project to see:
- Per-agent latency breakdown
- Token counts per LLM call  
- Tool call inputs/outputs (Tavily queries, ArXiv results)
- Full state at each node transition
- Error traces with full context if any agent fails

---

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `GROQ_API_KEY` | âœ… | [console.groq.com](https://console.groq.com) â€” free tier |
| `TAVILY_API_KEY` | âœ… | [tavily.com](https://tavily.com) â€” free tier |
| `LANGSMITH_API_KEY` | Recommended | [smith.langchain.com](https://smith.langchain.com) â€” free tier |
| `LANGSMITH_PROJECT` | Recommended | Set to `deep-research-engine` |
| `LANGSMITH_TRACING_V2` | Recommended | Set to `true` |
| `API_BASE` | Docker only | Auto-set to `http://api:8000` in compose |

---

## Related Projects

| Project | Pattern | What it proved |
|---------|---------|----------------|
| [MultiTool_Research](https://github.com/noviciusss/MultiTool_Research) | Single-agent ReAct | Tool use, conversation memory |
| DoCopilot | RAG + LLM-as-Judge | Document QA, evaluation pipelines |
| **Argus** (this) | Multi-agent Supervisor | Orchestration, async APIs, production deployment |

---

## License

MIT
